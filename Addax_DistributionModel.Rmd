---
title: "Species Distribution Modeling"
author: "Jared Stabach, Smithsonian Conservation Biology Institute"
date: '`r format(Sys.time(), "%d %B, %Y")`'
output:
  html_document:
    toc: true
    toc_depth: 3
    toc_float: 
      collapsed: false
      smooth_scroll: true
    number_sections: false
    #theme: united
    #highlight: tango
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

<a href="https://github.com/Smithsonian/Addax_DistributionModel.git" class="github-corner" aria-label="View source on GitHub"><svg width="80" height="80" viewBox="0 0 250 250" style="fill:#151513; color:#fff; position: absolute; top: 0; border: 0; right: 0;" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a><style>.github-corner:hover .octo-arm{animation:octocat-wave 560ms ease-in-out}@keyframes octocat-wave{0%,100%{transform:rotate(0)}20%,60%{transform:rotate(-25deg)}40%,80%{transform:rotate(10deg)}}@media (max-width:500px){.github-corner:hover .octo-arm{animation:none}.github-corner .octo-arm{animation:octocat-wave 560ms ease-in-out}}</style>

# Species Distribution Modeling

Addax (*Addax nasomaculatus*) are considered one of the rarest antelopes on earth, with an estimated population of <100 individuals in the wild. We assessed the distribution and occurrence of this species from field surveys collected by the [Sahara Conservation Fund](https://www.saharaconservation.org/) over a 7-year period (2008-2014). Our results provide insight into the factors contributing to species occurrence and are guiding field surveys to areas that have the potential to support small and geographically isolated populations of addax. We incorporated field-derived variables of vegetation cover with remote sensing measures of vegetation productivity (NDVI - [Normalized Difference Vegetation Index](https://earthobservatory.nasa.gov/Features/MeasuringVegetation/measuring_vegetation_2.php)) and surface roughness (derived from 30-m [SRTM](https://www2.jpl.nasa.gov/srtm/)). Models were fit in a generalized linear regression framework to evaluate and predict species occurrence.

![Addax moving across shifting sands in the Tin Toumma desert, Niger (Photo: T.Rabeil, SCF)](addax_termit_niger_0512_scf_rabeil_01.jpg)

# Lab Excercise

In this excercise, we will follow the steps detailed in:

Stabach, J.A., T. Rabeil, V. Turmine, T. Wacher, T. Mueller, and P. Leimgruber. 2017. On the brink of extinction - Habitat selection of addax and dorcas gazelle across the Tin Toumma desert, Niger. [Diversity and Distributions 23:581-591](./Publication/Stabach_etal_Addax_2017.pdf).

We will use [R](https://cran.r-project.org/) for all analyses to create a species distribution model, highlighting the variables important in predicting addax occurrence. Like other excercises in [R](https://cran.r-project.org/), we will first load the necessary packages to perform the analyses. Use `help()` for any operations that you don't understand or that require additional information.

## Load Packages
```{r Library,message=FALSE,warning=FALSE}
library(usdm)
library(arm)
library(visreg)
library(pROC)
library(DAAG)
library(fields)
library(MuMIn)
library(gstat)
library(sp)
library(lubridate)
library(DT)
library(leaflet)
```

## Data File

The raster data layers (NDVI and surface roughness) have already been extracted at plot locations (see [SurfaceRoughness](SurfaceRoughness.html)). This has been done to focus on the statistical modeling, rather that on the details to create the spatial dataset. Building the spatial database, however, is hugely important and should not be overlooked. The layers included in analyses will ultimately depend on your scientific research questions and what is biologically relevant for your species.

NDVI and surface roughness were summarized by calculating the mean value of all pixels within a 2.5-km radius of each plot location (a moving window analysis). Plot locations were spaced at 5-km intervals along line transects. Each line transect varied in length between 50-km and 100-km and were spaced ~10-km apart. Transects were repetitively sampled across years. 

All data (including animals counts) were summarized at plot locations. We re-coded animal counts within a 2.5-km radius to a measure of occurrence (i.e, presence/absence). Thus, we modeled the data as a series of 1's and 0's, representing addax occurrence at plot locations. Data were aggregated in this fashion because of variability between surveys (i.e., the transects locations didn't overlap exactly) and because we did not have confidence in the accuracy of the number of individuals recorded at each sighting. In addition, distance to animal sighting locations were only recorded in a subset of the surveys. Sightings >500-m from the transect were removed due to an assumed undercounting bias (confirmed by investigating the frequency of sighthings in relation to distance). This allowed for a conservative broad-scale approach to incorporate extremely messy field data collected over multiple years. See more details in [Stabach et al. 2017](./Publication/Stabach_etal_Addax_2017.pdf).

```{r Load, eval = T}
# Load SpatialPointsDataFrame.  Set working directory if necessary.
load(file="Addax_Dataset2.RData")
# Look at data
head(Sub.All2)

# Or get fancy with how the data table is displayed
#datatable(Sub.All2, rownames = FALSE, filter="top", options = list(pageLength = 5, scrollX = T))
```

Columns in the database include *Date* of survey, *X* and *Y* location of each plot, the number of addax and dorcas gazelle (a conspecific) sighted at each location, a unique plot ID (*Pt_ID*), and the presence/absence of vegetation species *Cornulaca monocantha* (Cornul), *Stipagrostis acutiflora* (Stipa1), and *Stipagrostis vulnerans* (Stipa2). These vegetation species were thought *a priori* to influence addax occurence and were collected at each plot location. *Human* disturbance (e.g., footprint, sighting, tire tracks) were also recorded (i.e., *Human* = 1). 

Other variables, including surface roughness (*Rough*) and the Normalized Difference Vegetation Index (*NDVI*) were included as data layers in our analysis.  Surface roughness is defined as the change in local elevation range (i.e., the difference between the minimum and maximum values of a cell and its eight surrounding neighbors). NDVI is known to be strongly correlated with a region's vegetation productivity/greenness and has been used extensively as an important parameter in models predicting animal movement and habitat use. NDVI data (MOD13Q1) was downloaded as 16-day cloud-free data composites with a 250-m resolution.

## Summarize Dataset

If we visualize the dataset, we see that data were collected multiple times a year and that the occupancy of addax (and dorcas gazelle) vary between year and season.

```{r Aggregate, eval=T, echo=F}
# Place data in a table to summarize results. 
Unique.ID <- unique(Sub.All2$YearMonth)

# Create matrix to hold everything
dat6 <- matrix(NA,nrow = length(Unique.ID),ncol = 9,dimnames = list(c(),c("YearMonth","Year","Season","PresAddax","PresDorc","PrevAdd","PrevDorc","One","Both")))

for (i in 1:length(Unique.ID)){
	temp <- subset(Sub.All2, YearMonth == Unique.ID[i])
		# Summarize dataset
		dat6[i,1] <- Unique.ID[i]
		dat6[i,2] <- as.character(unique(temp$Year))
		dat6[i,3] <- as.character(unique(temp$Season))
			obs.Add <- ifelse(temp$Addax > 0, 1, 0)
			obs.Dorc <- ifelse(temp$Dorcas > 0, 1, 0)
			Both <- obs.Add + obs.Dorc
			Both2 <- ifelse(Both > 1, 1, 0) # Vector where both present
			One <- ifelse(Both > 0, 1, 0) # At least one species present
		dat6[i,4] <- sum(obs.Add)
		dat6[i,5] <- sum(obs.Dorc)
		dat6[i,6] <- round(sum(obs.Add)/length(obs.Add)*100,digits=1)
		dat6[i,7] <- round(sum(obs.Dorc)/length(obs.Add)*100,digits=1)
		dat6[i,8] <- round(sum(One)/length(obs.Add)*100,digits=1)
		dat6[i,9] <- round(sum(Both2)/length(obs.Add)*100,digits=1)
}

# Look at result.  This should match Table 1 in Stabach et al. 2017.
dat6 <- as.data.frame(dat6)
dat6

# Write to file, if necessary
#write.csv(dat6,file="Addax_Dorcas_Prevalence.csv", quote = FALSE, row.names = FALSE)
```

## Scaling Parameter Values

It is often helpful and necessary to scale continuous parameters that have vastly different value ranges (e.g., elevation and NDVI). Doing so can help with model convergence. While conclusions will remain the same, it is important to remember that data are not on the same scale as the original values and must be back-transformed when making raster predictions.  This is **critical**.  

**Questions:**

1) How would you find out the details of the `scale` function?  
2) What is the function doing?  
3) Can you write out the function when `scale` and `center` are **TRUE**?

```{r ScaleAnswer, eval=F, echo=F}
# Must specify x as the column header
# Function will then center (subtract the mean) and scale (divide by the standard deviation) every value
CenterScale <- function(x){
  (x-mean(x))/sd(x)
}

# Test both methods
test <- as.numeric(scale(Sub.All2$TRI, center=TRUE))
summary(test)
test2 <- CenterScale(Sub.All2$TRI)
summary(test2)
```

```{r Scale, eval=TRUE}
# Scale the parameters...but let's create a copy of the dataset first
Sub.Scale <- Sub.All2

# Use the help(scale) for more information on the function
Sub.Scale$shuman <- as.numeric(scale(Sub.Scale[,"Human"],center=TRUE))
Sub.Scale$sndvi <- as.numeric(scale(Sub.Scale[,"ndvi"],center=TRUE))
Sub.Scale$srough <- as.numeric(scale(Sub.Scale[,"ROUGH"],center=TRUE))
Sub.Scale$sDorcas <- as.numeric(scale(Sub.Scale[,"Dorcas"],center=TRUE))
Sub.Scale$sAddax <- as.numeric(scale(Sub.Scale[,"Addax"],center=TRUE))
```

## Saving Parameter Values

Now that the parameter values have been scaled, we will use these values for modeling purposes.  But, when we make the prediction, we will need to back-transform our raster surfaces.  Otherwise, the coefficients from our models can't be applied to the raster grids.  To do so, we need the mean and standard deviation of each column.  We will then use these values to scale and center each of the raster surfaces.

In this initial step, we create a blank matrix to hold the values.  Later in the code, we will refer to the `Scale.Val` object. 

```{r ScaleHold, eval=TRUE}
# Create a blank matrix to hold the Mean and Standard Deviation of each continuous variable included the dataset. 
# These values are essential to rescale to original values
Scale.Val <- matrix(NA, 2, ncol(Sub.Scale[,c(15,17,8,12,7)]), dimnames=list(c("Mn","Sd"),c("Rough","NDVI","Dorcas","Human","Addax")))

# Calculate the mean and standard deviations.
Scale.Val[1,] <- apply(Sub.Scale[,c(15,17,8,12,7)],2,mean)
Scale.Val[2,] <- apply(Sub.Scale[,c(15,17,8,12,7)],2,sd)

# Look at values
Scale.Val

# Create a second object which summarize the data.  Here, I'm just specifying the column numbers.
data.scale <- scale(Sub.Scale[,c(15,17,8,12,7)],center=TRUE)
(summary.data <- apply(data.scale,2,summary))
```

## Correlation Analysis

Like other analyses, we need to evalute data redundancy.  Are any of the continuous variables included in our analysis highly correlated?  

**Questions:**

1) What do we do with categorical variables?
2) What is a reasonable correlation threshold?

```{r Corr, eval=T}
# Group variables together in a dataframe
data.all <- as.data.frame(cbind(Sub.Scale$srough,Sub.Scale$sndvi,Sub.Scale$sDorcas,Sub.Scale$shuman,Sub.Scale$sAddax))

# Variance Inflation Analysis
(eval.vif <- vifstep(data.all))

# See also cor(data.all)
```

# Data Model

## Generalized Linear Regression (GLM)

Model the occurrence of addax in a Generalized Linear Regression (GLM) framework. Our goal *here* was not necessarily to create the very best model. Instead, we aimed to:

1. Identify the response of all variables at predicting addax occurrence
2. Evaluate a submodel that contains only the remote sensing layers to make a prediction of habitat suitability across the landscape 

We can easily assess how this 'submodel' compares with our 'full' model or the 'best' model. 

**Question:**

1) Why is using a GLM advantageous given the data and objectives?
2) When fitting multiple models, how should we evaluate which of the models is the best (Hint: See the `dredge` function in the `library(MuMIn)`?

```{r ModelDregde, eval=F, echo=T}
options(na.action=na.fail)
m1 <- dredge(glm.Addax)
head(m1)
```

```{r Model, eval=T}
# Create a full model with all the variables you think are important predictors of addax occurrence
glm.Addax <- glm(obsAddax ~ srough + I(srough^2) + sndvi + I(sndvi^2) + Human + obsDorc + Stipa1 + Stipa2 + Cornul + Season + Year, data = Sub.Scale, family = binomial(link="logit"))
# Summarize result
summary(glm.Addax)

# Or print the confidence intervals
#confint(glm.Addax)
```

## Graph Results

Use the `visreg` and `coefplot` functions to easily graph the results from a glm.

```{r ModelGraph, eval=T}

# Graph result for surface roughness
visreg(glm.Addax,"srough",scale="response",ylab="Prob",partial=TRUE,line=list(col="blue"),fill=list(col="gray"),ylim=c(0,1))

# Plot the coefficients
coefplot(glm.Addax, plot=TRUE, mar=c(1,4,5.1,2), intercept=FALSE, vertical=TRUE, main="", var.las=1, frame.plot=FALSE)
```

**Question:**

1) How could we plot all the variables included in the model or change which variable was output?

```{r PlotAll, eval=F, echo=F}
visreg(glm.Addax, scale="response",partial=TRUE,line=list(col="blue"),fill=list(col="gray"),ylim=c(0,1))
visreg(glm.Addax,"sndvi", scale="response",partial=TRUE,line=list(col="blue"),fill=list(col="gray"),ylim=c(0,1))
```

### Making the Plot Look Nicer

Can we improve the look of these plots and plot the x-axis values with the original values?    

mportantly, we need to Here again, look at the `help(visreg)` to gather information on the items that can be changed/updated. This 

```{r PrettyPlot, eval=T}
# Graph the NDVI response
# ************************
# Since graphing NDVI, require the 2nd column of the minimum and maximum  values calculated above in the summary.data table
MinVal <- summary.data[1,2]
MaxVal <- summary.data[6,2]

    # Then, set the sequence in which to plot the values 
    divs <- 100
    x <- seq(MinVal, MaxVal, length.out=divs)
    x.unscale <- x*Scale.Val[2,2]+Scale.Val[1,2] # Need to keep track of the correct row and column.  Here, multiply by the SD and add the mean

# You can then update the unscaled values into the plot, by specifying the axis with these values.
visreg(glm.Addax,"sndvi",scale="response",ylab="Probability of Occurrence",xlab="NDVI",partial=TRUE, axes=FALSE, rug=0, ylim=c(0,1),line=list(col="black",lwd=2,lty=1),fill=list(col="grey"),points=list(col="black",cex=0.25,pch=19),frame=FALSE,main="Addax")

# Plot axes
axis(2,col="black",col.axis="black")
# Most complicated part......Are just substituting the transformed values for the original data.
axis(1,at=c(x[1],x[50],x[100]),lab=c(round(x.unscale[1],digits=2),round(x.unscale[50],digits=2),round(x.unscale[100],digits=2)),col="black",col.axis="black")

# That's nicer!
```

**Question:**

1) How would you plot the response to surface roughness?
1) This code for plotting surface roughness is nearly identical to the code for plotting NDVI.  Could you automate this and programmatically plot both graphs?

```{r PrettyPlotSrough, eval=F, echo=F}
# Really, all you need to do is recognize that you need to reference a different column in the summary.data and Scale.Val objects.
# Change the column values below
MinVal <- summary.data[1,1] # Note the change in column number
MaxVal <- summary.data[6,1] # Note the change in column number

  # Then, set the sequence in which to plot the values 
  divs <- 100
  x <- seq(MinVal, MaxVal, length.out=divs)
  x.unscale <- x*Scale.Val[2,1]+Scale.Val[1,1] # Note the change in column number

# Now, you simply need to change the variable that you want to plot ('srough') and the x label (xlab)
visreg(glm.Addax,"srough",scale="response",ylab="Probability of Occurrence",xlab="Surface Roughness",partial=TRUE, axes=FALSE, rug=0, ylim=c(0,1),line=list(col="black",lwd=2,lty=1),fill=list(col="grey"),points=list(col="black",cex=0.25,pch=19),frame=FALSE,main="Addax")

# Plot axes
axis(2,col="black",col.axis="black")
axis(1,at=c(x[1],x[50],x[100]),lab=c(round(x.unscale[1],digits=2),round(x.unscale[50],digits=2),round(x.unscale[100],digits=2)),col="black",col.axis="black")

# Now loop over both variables and plot
# **************************************
# **************************************

# Create variables that you want to print iteratively
plot.variable <- c("srough","sndvi")
main.label <- c("Roughness","NDVI") 
x.label <- c("Surface Rougness", "NDVI")

for(i in 1:length(plot.variable)){

  # Change the column number to [i]...this is what you will be looping through
  MinVal <- summary.data[1,i] 
  MaxVal <- summary.data[6,i] 

    # Then, set the sequence in which to plot the values 
    divs <- 100
    x <- seq(MinVal, MaxVal, length.out=divs)
    # Change the column to [i]
    x.unscale <- x*Scale.Val[2,i]+Scale.Val[1,i]

# Now change the variables in the plot
visreg(glm.Addax,plot.variable[i],scale="response",ylab="Probability of Occurrence",xlab=x.label[i],partial=TRUE, axes=FALSE, rug=0, ylim=c(0,1),line=list(col="black",lwd=2,lty=1),fill=list(col="grey"),points=list(col="black",cex=0.25,pch=19),frame=FALSE,main=main.label[i])

# Plot axes
axis(2,col="black",col.axis="black")
axis(1,at=c(x[1],x[50],x[100]),lab=c(round(x.unscale[1],digits=2),round(x.unscale[50],digits=2),round(x.unscale[100],digits=2)),col="black",col.axis="black")
}
```

## Validation

How do we determine if our models are any good? We can compare our full model to a null model and look at the Area Under the Curve (AUC) statistic. AUC compares the difference between the True Positive Classification Rate and a False Positive Rate (i.e., Specificity vs Sensitivity).  Some guidelines to AUC:

* 0.9 - 1: Excellent (A)
* 0.8 - 0.9: Good (B)
* 0.7 - 0.8: Fair (C)
* 0.6 - 0.7: Poor (D)
* 0.5 - 0.6: Fail (F)

```{r Val, eval=T}
# What's the AUC?
predpr <- predict(glm.Addax, type=c("response"))
(roccurve <- roc(Sub.Scale$obsAddax ~ predpr))
plot(roccurve)
```

# Raster Prediction

One of the most valuable parts of a species distribution model is predicting to locations where surveys were not performed. In order to make a prediction at these locations, we need data that has wall-to-wall coverage. Unfortunately, only two of our layers incorporated in the full model have full coverage (*NDVI* and *Surface Roughness*). 

Create a model with these two layers, assessing how the model compares with the full model and predict across the entire study area. As you will see, model statistics indicate that this sub-model is not as good as the full model (compare the AIC, AUC).  But, perhaps still useful to help guide field teams. 

The model is still useful, however, as long as we are clear about its shortcomings (i.e., we'd expect the predictive power to be decreased since we are not including the fine scale data collected at individual plot locations). 

## Create Model Subset and Evaluate

```{r ModelSub, eval=T}
glm.Addax2 <- glm(obsAddax ~ srough + I(srough^2) + sndvi + I(sndvi^2), data = Sub.Scale, family = binomial(link="logit"))

# Summarize and print confidence intervals
summary(glm.Addax2)

# AUC
predpr <- predict(glm.Addax2, type=c("response"))
(roccurve <- roc(Sub.Scale$obsAddax ~ predpr))
plot(roccurve)
```

## Load Raster Layers

Load the NDVI data and SRTM data to make a model prodection. 

**Remember**, that we still will need to re-scale the raster layers, since our model results (our coefficients) are based on data that was also scaled.  We will use an NDVI image that was available from November 2007.  This was done because we also have an ancillary dataset (i.e., flight survey) that we used as an external dataset to assess model performance (data not shown).

### NDVI

```{r Predict, eval=T}
# Load NDVI data from flight survey date
# Note that this file has 250-meter resolution (MOD13Q1 data product)
# The SRTM data has 30-m resolution
# We need these data sources to have the same resolution in order to make the prediction. 

# Use the 2007 data from November for validation...matches the flight survey
ndvi <- raster("Data/MOD13Q1_Nov2017.tif")
# Convert raster to values to actual NDVI values
ndvi <- ndvi*0.0001

# Data needs to be summarized at 2.5km and scaled to match
# Create a focal grid....to match the resampling done at the survey points.....this just creates a matrix
# This is confusing, but it is just a weighted grid...to summary values within the grid
FoGrid <- focalWeight(ndvi,d=2500,type='circle')

# Now Summarize the NDVI within the focalWeight grid
ndvi2 <- focal(x=ndvi,w=FoGrid,fun=sum,na.rm=TRUE) # Need to use sum....because the focalWeight grid...creates a matrix of values that add to 1.  We want a summary of values within the focal grid

# Plot result
plot(ndvi2)
```

### Surface Roughness

Now do the same procedure for the *Surface Roughness* layer.  *Rough* is a 30-m resolution file, so will take longer to process.

```{r SRTM, eval=T}
# Create a different focalWeight grid because the cell resolutions are different (30 meters instead of 250 meters)
rough <- raster("Data/Rough_Sub.tif")

FoGrid1 <- focalWeight(rough,d=2500,type='circle')
rough2 <- focal(x=rough,w=FoGrid1,fun=sum,na.rm=TRUE)

# Plot layer
plot(rough2)
```

### Scale Rasters and Resample

Scale the rasters using the value summaries (mean and standard deviation) created above. Then, the NDVI raster file needs to be resampled to match the SRTM datafile. The extent of these files also needs to match.

**Question:**

1) What interpolation method should you use when resampling the raster dataset?

```{r Scale2, eval=T}
# Scale values. To back transform, you need to:x - mean(x) / sd(x))
Scale.Val
srough <- (rough2-Scale.Val[1,1])/Scale.Val[2,1]
sndvi <- (ndvi2-Scale.Val[1,2])/Scale.Val[2,2]

# Resample the grids so that they can be added together in the model.  
# This may take a long time if interpolating from 250 to 30 m
# Much quicker if going to 30 to 250 m
# Why would you want to do one resolution over the other?
#sndvi <- resample(sndvi,srough,method="bilinear") # 250 - 30 m (Slow)
srough <- resample(srough, sndvi, method="bilinear") # 30 - 250 m (Fast)

# Compare the resolutions 
compareRaster(srough,sndvi)
```

### Make Final Prediction

Now that the rasters have been scaled, we can make a final prediction.  This can be accomplished by using the coefficients from the model and manually inputting them into the equation.  Alternatively, we can use the `predict` command.  Using the `predict` command requires that we create a `rasterBrick` with the same variable names as included in our model.

```{r FinalPredict, eval=T}
# Summarize the model
summary(glm.Addax2)

# Manually:
# We could physically calculate the prediction from the model coefficients:
##coef <- summary(glm.Addax2)
##coef <- coef$coefficients

##Addax.predict <- (exp(coef[1] + rgh.scale*coef[2] + rgh.scale^2*coef[3] + ndvi.rsmp*coef[4] + ndvi.rsmp^2*coef[5])/(1 + exp(coef[1] + rgh.scale*coef[2] + rgh.scale^2*coef[3] + ndvi.rsmp*coef[4] + ndvi.rsmp^2*coef[5])))

# Using Predict:
# Create quadratic raster layers to include in the rasterBrick
srough2 <- srough^2
sndvi2 <- sndvi^2

# Add to brick and rename layer names
satImage <- brick(srough, srough2, sndvi, sndvi2)
names(satImage) <- c("srough","srough2", "sndvi", "sndvi2")

# Predict and export image to directory
Addax.predict <- predict(satImage, glm.Addax2, type="response", progress='text')

# Plot result
plot(Addax.predict)

# Or write the raster to the directory separately
#writeRaster(Addax.predict, 'Test.tif', format="GTiff", datatype = "INT1U", overwrite=TRUE)
```

### Plot Presence Threshold

What if I only wanted to see values above a certain threshold?  Using simple raster math, we could visualize only the areas that meet our criteria.

```{r Threshold, eval=T}
# Create presence threshold
presenceThresh <- 0.13

# Threshold the result and plot
Addax.Thresh <- Addax.predict >= presenceThresh
plot(Addax.Thresh)
```

## Interactive Mapping

Lastly, we can also take this one step further and plot our predicted surface on an interactive map, so that we have a better idea of its context in the real world and also provide a way to post our results on relevant social media.

```{r Interactive, eval=T}
# Create color ramp
pal <- colorNumeric(rev(terrain.colors(10)), values(Addax.predict), na.color = "transparent")

# Reproject raster predict to Lat/Long
proj.info <- crs(Addax.predict)
proj.info.LongLat <- CRS("+proj=longlat +datum=WGS84")
Addax.predict.lat <- projectRaster(Addax.predict, crs=proj.info.LongLat) # Use default resolution

leaflet() %>%
  addTiles() %>%
  addRasterImage(Addax.predict, colors = pal, opacity = 0.9) %>%
  addLegend(pal = pal, values = values(Addax.predict),
    title = "Predicted Addax Occurrence")
```