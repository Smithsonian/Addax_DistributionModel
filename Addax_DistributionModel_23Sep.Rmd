---
title: "Species Distribution Modeling"
author: "Jared Stabach, Smithsonian Conservation Biology Institute"
date: '`r format(Sys.time(), "%d %B, %Y")`'
output:
  html_document:
    toc: true
    toc_depth: 3
    toc_float: 
      collapsed: false
      smooth_scroll: true
    number_sections: false
    #theme: united
    #highlight: tango
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Addax distribution

Addax (*Addax nasomaculatus*) are considered one of the rarest antelopes on earth, with an estimated population of **<100 individuals** in the wild. We assessed the distribution and occurrence of this species from field surveys collected by the [Sahara Conservation Fund](https://www.saharaconservation.org/) over a 7-year period (2008-2014). Our results provide insight into the factors contributing to species occurrence and are guiding field surveys to areas that have the potential to support small and geographically isolated populations of addax. We incorporated field-derived variables of vegetation cover with remote sensing measures of vegetation productivity (NDVI - [Normalized Difference Vegetation Index](https://earthobservatory.nasa.gov/Features/MeasuringVegetation/measuring_vegetation_2.php)) and surface roughness (derived from 30-m [SRTM](https://lta.cr.usgs.gov/SRTM)). Models were fit in a generalized linear regression framework to predict and evaluate species occurrence.

## Lab Excercise

In this excercise, we will follow the steps detailed in:

Stabach, J.A., T. Rabeil, V. Turmine, T. Wacher, T. Mueller, and P. Leimgruber. 2017. On the brink of extinction - Habitat selection of addax and dorcas gazelle across the Tin Toumma desert, Niger. Diversity and Distributions 23:581-591.

We will use [R](https://cran.r-project.org/) for all analyses to create a species distribution model, highlighting the variables important in predicting addax occurrence. Like other excercises in [R](https://cran.r-project.org/), we will first load the necessary packages to perform the analyses. Use `help()` for any operations that you don't understand or that require additional information.

### Load Packages
```{r Library,message=FALSE,warning=FALSE}
library(usdm)
library(arm)
library(visreg)
library(pROC)
library(DAAG)
library(fields)
library(MuMIn)
library(gstat)
library(sp)
library(lubridate)
```

### Data File

The raster data layers (NDVI and surface roughness) have already been extracted at plot locations. This has been done to focus on modeling the statistical relationships in this exercise. Building the spatial database, however, is hugely important and should not be overlooked. The layers included in analyses will ultimately depend on your scientific research questions and what is biologically relevant for your species.

NDVI and surface roughness were summarized by calculating the mean value of all pixels within a 2.5-km radius of each plot location (a moving window analysis). Plot locations were spaced at 5-km intervals along line transects. Each line transect varied in length between 50-km and 100-km and were spaced ~10-km apart. Transects were repetitively sampled across years. Information on how to create and extract surface roughness is included as an addendum to this exercise.

All data (including animals counts) were summarized at plot locations. We re-coded animal counts within a 2.5-km radius to a measure of occurrence (i.e, presence/absence). Thus, we modeled the data as a series of 1's and 0's, representing addax occurrence at plot locations. Data were aggregated in this fashion because of variability between surveys (i.e., the transects locations didn't overlap exactly) and because we did not have confidence in the accuracy of the number of individuals recorded at each sighting. In addition, distance to animal sighting locations were only recorded in a subset of the surveys. Sightings >500-m from the transect were removed due to an assumed undercounting bias (confirmed by investigating the frequency of sighthings in relation to distance). This allowed for a conservative broad-scale approach to incorporate extremely messy field data collected over multiple years. See more details in Stabach et al. 2017.

```{r Load, eval = T}
# Load SpatialPointsDataFrame.  Set working directory if necessary.
load(file="Addax_Dataset2.RData")
# Look at data
head(Sub.All2)
```

Columns in the database include *Date* of survey, *X* and *Y* location of each plot, the number of addax and dorcas gazelle (a conspecific) sighted at each location, a unique plot ID (*Pt_ID*), and the presence/absence of vegetation species *Cornulaca monocantha* (Cornul), *Stipagrostis acutiflora* (Stipa1), and *Stipagrostis vulnerans* (Stipa2). These vegetation species were thought *a priori* to influence addax occurence and were collected at each plot location. *Human* disturbance (e.g., footprint, sighting, tire tracks) were also recorded (i.e., *Human* = 1). 

Surface roughness (*Rough*) is defined as the change in local elevation range (i.e., the difference between the minimum and maximum values of a cell and its eight surrounding neighbors). *NDVI* (Normalized Difference Vegetation Index) is known to be strongly correlated with a region's vegetation productivity/greenness and has been used extensively as an important parameter in models predicting animal movement and habitat use. NDVI data (MOD13Q1) was downloaded as 16-day cloud-free data composites with a 250-m resolution.

### Summarize Dataset

Execute a query to estimate the number of surveys conducted each year. Graph the data to visualize the patterns. How would you change the plotting function to visualize the extent of every survey?

```{r Aggregate, eval=T}
# Place data in a table to summarize results. 
Unique.ID <- unique(Sub.All2$YearMonth)

# Create matrix to hold everything
dat6 <- matrix(NA,nrow = length(Unique.ID),ncol = 9,dimnames = list(c(),c("YearMonth","Year","Season","PresAddax","PresDorc","PrevAdd","PrevDorc","One","Both")))

for (i in 1:length(Unique.ID)){
	temp <- subset(Sub.All2, YearMonth == Unique.ID[i])
		# Summarize dataset
		dat6[i,1] <- Unique.ID[i]
		dat6[i,2] <- as.character(unique(temp$Year))
		dat6[i,3] <- as.character(unique(temp$Season))
			obs.Add <- ifelse(temp$Addax > 0, 1, 0)
			obs.Dorc <- ifelse(temp$Dorcas > 0, 1, 0)
			Both <- obs.Add + obs.Dorc
			Both2 <- ifelse(Both > 1, 1, 0) # Vector where both present
			One <- ifelse(Both > 0, 1, 0) # At least one species present
		dat6[i,4] <- sum(obs.Add)
		dat6[i,5] <- sum(obs.Dorc)
		dat6[i,6] <- round(sum(obs.Add)/length(obs.Add)*100,digits=1)
		dat6[i,7] <- round(sum(obs.Dorc)/length(obs.Add)*100,digits=1)
		dat6[i,8] <- round(sum(One)/length(obs.Add)*100,digits=1)
		dat6[i,9] <- round(sum(Both2)/length(obs.Add)*100,digits=1)
}

# Look at result.  This should match Table 1 in Stabach et al. 2017.
dat6 <- as.data.frame(dat6)
dat6

# Write to file, if necessary
#write.csv(dat6,file="Addax_Dorcas_Prevalence.csv", quote = FALSE, row.names = FALSE)
```

### Scaling Parameter Values

It is often helpful and necessary to scale continuous parameters that have vastly different value ranges (e.g., elevation and NDVI). Doing so can help with model convergence. While conclusions will remain the same, it is important to remember that data are not on the same scale as the original values and must be back-transformed when making raster predictions.  This is critical.

```{r Scale, eval=TRUE}
# Scale the parameters...Create a copy of the dataset
Sub.Scale <- Sub.All2

# Use the help(scale) for more information on the function
Sub.Scale$shuman <- as.numeric(scale(Sub.Scale[,"Human"],center=TRUE))
Sub.Scale$sndvi <- as.numeric(scale(Sub.Scale[,"ndvi"],center=TRUE))
Sub.Scale$srough <- as.numeric(scale(Sub.Scale[,"ROUGH"],center=TRUE))
Sub.Scale$sDorcas <- as.numeric(scale(Sub.Scale[,"Dorcas"],center=TRUE))
Sub.Scale$sAddax <- as.numeric(scale(Sub.Scale[,"Addax"],center=TRUE))

# Create a blank matrix to hold the Mean and Standard Deviation of each continuous variable included the dataset. 
# These values are essential to rescale to original values
Scale.Val <- matrix(NA, 2, ncol(Sub.Scale[,c(15,17,8,12,7)]), dimnames=list(c("Mn","Sd"),c("Rough","NDVI","Dorcas","Human","Addax")))

# Calculate the mean and standard deviations.
Scale.Val[1,] <- apply(Sub.Scale[,c(15,17,8,12,7)],2,mean)
Scale.Val[2,] <- apply(Sub.Scale[,c(15,17,8,12,7)],2,sd)

# Look at values
Scale.Val
```

### Correlation Analysis

Evalute if we have data redundancy.  Are there any continuous variables with high levels of correlation?  What is a reasonable correlation threshold?

```{r Corr, eval=T}
# Group variables together in a dataframe
data.all <- as.data.frame(cbind(Sub.Scale$srough,Sub.Scale$sndvi,Sub.Scale$sDorcas,Sub.Scale$shuman,Sub.Scale$sAddax))

# Variance Inflation Analysis
(eval.vif <- vifstep(data.all))
```

## Data Model

### Generalized Linear Regression (GLM)

Model the occurrence of addax in a Generalized Linear Regression (GLM) framework. Our goal here is not necessarily to create the very best model. Instead, we aimed to identify the response of all variables at predicting addax occurrence. We then want to evaluate a submodel that contains only the remote sensing layers to make a prediction of habitat suitability across the landscape. We can easily assess how this 'submodel' compares with our 'full' model or the 'best' model. Why is using a GLM advantageous given the data and objectives?

```{r Model, eval=T}
# Create a full model with all the variables you think are important predictors of addax occurrence
glm.Addax <- glm(obsAddax ~ srough + I(srough^2) + sndvi + I(sndvi^2) + Human + obsDorc + Stipa1 + Stipa2 + Cornul + Season + Year, data = Sub.Scale, family = binomial(link="logit"))
# Summarize result and look at the confidence intervals
summary(glm.Addax)

# Graph result for surface roughness
visreg(glm.Addax,"srough",scale="response",ylab="Prob",partial=TRUE,line=list(col="blue"),fill=list(col="gray"),ylim=c(0,1))

# Plot the coefficients
coefplot(glm.Addax, plot=TRUE, mar=c(1,4,5.1,2), intercept=FALSE, vertical=TRUE, main="", var.las=1, frame.plot=FALSE)
```


### Validation

How good are the models? We can compare our full model to a null model, look at the Area Under the Curve (AUC) statistics. AUC compares the difference between the True Positive Classification Rate and a False Positive Rate (i.e., Specificity vs Sensitivity). Some guidelines to AUC:

* 0.9 - 1: Excellent (A)
* 0.8 - 0.9: Good (B)
* 0.7 - 0.8: Fair (C)
* 0.6 - 0.7: Poor (D)
* 0.5 - 0.6: Fail (F)

```{r Val, eval=T}
# What's the AUC?
predpr <- predict(glm.Addax, type=c("response"))
(roccurve <- roc(Sub.Scale$obsAddax ~ predpr))
plot(roccurve)
```

## Raster Prediction

One of the most valuable parts of a species distribution model is predicting to locations where surveys were not performed. In order to make a prediction at these locations, we need data that has wall-to-wall coverage. Unfortunately, only two of our layers incorporated in the full model have full coverage (*NDVI* and *Surface Roughness*). 

Create a model with these two layers, assessing how the model compares with the full model and predict across the entire study area. As you will see, model statistics indicate that this sub-model is not as good as the full model (compare the AIC, AUC). 

The model is still useful, however, as long as we are clear about its shortcomings (i.e., we'd expect the predictive power to be decreased since we are not including the fine scale data collected at individual plot locations). 

### Create Model Subset and Evaluate

```{r ModelSub, eval=T}
glm.Addax2 <- glm(obsAddax ~ srough + I(srough^2) + sndvi + I(sndvi^2), data = Sub.Scale, family = binomial(link="logit"))

# Summarize and print confidence intervals
summary(glm.Addax2)

# AUC
predpr <- predict(glm.Addax2, type=c("response"))
(roccurve <- roc(Sub.Scale$obsAddax ~ predpr))
plot(roccurve)
```

### Load Raster Layers

Load the NDVI data and SRTM data to make a model prodection. Remember that we will need to re-scale the raster layers, since our model results (our coefficients) are based on data that was also scaled.  We will use an NDVI image that was available from November 2007.  This was done because we also have an ancillary dataset (i.e., flight survey) that we used as an external dataset to assess model performance (not shown).

#### NDVI

```{r Predict, eval=T}
# Load NDVI data from flight survey date
# Note that this file has 250-meter resolution (MOD13Q1 data product)
# The SRTM data has 30-m resolution
# We need these data sources to have the same resolution in order to make the prediction. Otherwise, we will get an error.

# Use the 2007 data from November for validation...matches the flight survey
ndvi <- raster("Data/MOD13Q1_Nov2017.tif")
# Convert raster to values to actual NDVI values
ndvi <- ndvi*0.0001

# Data needs to be summarized at 2.5km and scaled to match
# Create a focal grid....to match the resampling done at the survey points.....this just creates a matrix
# This is confusing, but it is just a weighted grid...to summary values within the grid
FoGrid <- focalWeight(ndvi,d=2500,type='circle')

# Now Summarize the NDVI within the focalWeight grid
ndvi2 <- focal(x=ndvi,w=FoGrid,fun=sum,na.rm=TRUE) # Need to use sum....because the focalWeight grid...creates a matrix of values that add to 1.  We want a summary of values within the focal grid

# Plot result
plot(ndvi2)
```

#### Surface Roughness

Now do the same procedure for the *Surface Roughness* layer.  *Rough* is a 30-m resolution file, so will take a longer time to process.

```{r SRTM, eval=T}
# Create a different focalWeight grid because the cell resolutions are different (30 meters instead of 250 meters)
rough <- raster("Data/Rough_Sub.tif")

FoGrid1 <- focalWeight(rough,d=2500,type='circle')
rough2 <- focal(x=rough,w=FoGrid1,fun=sum,na.rm=TRUE)

# Plot layer
plot(rough2)
```

#### Scale Rasters and Resample

Scale the rasters using the values summaries created above. Then, the NDVI raster file needs to be resampled to match the SRTM datafile. The extent of these files also needs to match.

```{r Scale2, eval=T}
# Scale values. To back transform, you need to:x - mean(x) / sd(x))
Scale.Val
srough <- (rough2-Scale.Val[1,1])/Scale.Val[2,1]
sndvi <- (ndvi2-Scale.Val[1,2])/Scale.Val[2,2]

# Resample the grids so that they can be added together in the model.  
# This may take a long time if interpolating from 250 to 30 m
# Much quicker if going to 30 to 250 m
#sndvi <- resample(sndvi,srough,method="bilinear") # 250 - 30 m (Slow)
srough <- resample(srough, sndvi, method="bilinear") # 30 - 250 m (Fast)

# Compare the resolutions 
compareRaster(srough,sndvi)
```

#### Make Final Prediction

Now that the rasters have been scaled, we can make a final prediction.  This can be accomplished by using the coefficients from the model and manually inputting them into the equation.  Alternatively, we can use the `predict` command.  Using the `predict` command requires that we create a `rasterBrick` with the same variable names as included in our model.

```{r FinalPredict, eval=T}
# Summarize the model
summary(glm.Addax2)

# Manually:
# We could physically calculate the prediction from the model coefficients:
##coef <- summary(glm.Addax2)
##coef <- coef$coefficients

##Addax.predict <- (exp(coef[1] + rgh.scale*coef[2] + rgh.scale^2*coef[3] + ndvi.rsmp*coef[4] + ndvi.rsmp^2*coef[5])/(1 + exp(coef[1] + rgh.scale*coef[2] + rgh.scale^2*coef[3] + ndvi.rsmp*coef[4] + ndvi.rsmp^2*coef[5])))

# Using Predict:
# Create quadratic raster layers to include in the rasterBrick
srough2 <- srough^2
sndvi2 <- sndvi^2

# Add to brick and rename layer names
satImage <- brick(srough, srough2, sndvi, sndvi2)
names(satImage) <- c("srough","srough2", "sndvi", "sndvi2")

# Predict and export image to directory
Addax.predict <- predict(satImage, glm.Addax2, type="response", progress='text')

# Plot result
plot(Addax.predict)

# Or write the raster to the directory separately
#writeRaster(Addax.predict, 'Test.tif', format="GTiff", datatype = "INT1U", overwrite=TRUE)
```
